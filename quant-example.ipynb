{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f96e5605",
   "metadata": {},
   "source": [
    "# Train a model for MNIST without quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3519efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4560d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train,y_train),(X_test,y_test)=keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3b30d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d0504b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6df7b307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "919a52be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x161a6418a00>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i= 13\n",
    "plt.matshow(X_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dafef91a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(),plt.imshow(X_train[i])\n",
    "plt.savefig(\"./6-1.jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22a7471d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da5906a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train/255\n",
    "X_test=X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "127fad24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "X_train_flattened=X_train.reshape(len(X_train),28*28)\n",
    "X_test_flattened=X_test.reshape(len(X_test),28*28)\n",
    "print(X_train_flattened.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa506833",
   "metadata": {},
   "source": [
    "# Using Flatten layer so that we don't have to call .reshape on input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e6568aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 2.3316 - accuracy: 0.8379\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.4355 - accuracy: 0.9013\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.3163 - accuracy: 0.9232\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2657 - accuracy: 0.9346\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2340 - accuracy: 0.9412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x161a1903af0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28,28)),\n",
    "    keras.layers.Dense(100,activation='relu'),\n",
    "    keras.layers.Dense(10,activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train,y_train,epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e199501",
   "metadata": {},
   "source": [
    "Evaluate baseline test accuracy and save the model for later usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6ea80c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 1ms/step - loss: 0.2536 - accuracy: 0.9354\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.25364774465560913, 0.9354000091552734]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "650cdd48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_model\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"./saved_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4d1102",
   "metadata": {},
   "source": [
    "# (1)Post Training Quantization PTQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7f26fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319944\n"
     ]
    }
   ],
   "source": [
    "converter=tf.lite.TFLiteConverter.from_saved_model(\"./saved_model\")\n",
    "tflite_model=converter.convert()\n",
    "print(len(tflite_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1447c07",
   "metadata": {},
   "source": [
    "# 量化权重\n",
    "权重可能会转换为精度降低的类型，例如 16 位浮点数或 8 位整数。我们通常建议将 16 位浮点数用于 GPU 加速，而将 8 位整数用于 CPU 执行。\n",
    "\n",
    "例如，下面给出了指定 8 位整数权重量化的方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e6a585e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84816\n"
     ]
    }
   ],
   "source": [
    "converter=tf.lite.TFLiteConverter.from_saved_model(\"./saved_model\")\n",
    "converter.optimizations=[tf.lite.Optimize.DEFAULT]\n",
    "tflite_quant_model=converter.convert()\n",
    "print(len(tflite_quant_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4539eb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./quant_model/tflite_model.tflite\",\"wb\")as f:\n",
    "    f.write(tflite_model)\n",
    "    \n",
    "with open(\"./quant_model/tflite_quant_model.tflite\",\"wb\")as f:\n",
    "    f.write(tflite_quant_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7590840b",
   "metadata": {},
   "source": [
    "推理时，最关键的密集部分使用 8 位而不是浮点数进行计算。与下面对权重和激活进行量化相比，存在一些推理时间性能开销。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294751fa",
   "metadata": {},
   "source": [
    "# 权重和激活的全整数量化\n",
    "通过确保量化权重和激活，可以改善延迟、处理时间和功耗，并访问仅支持整数的硬件加速器。这需要一个较小的代表性数据集。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f27a6381",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_calibration_steps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m converter\u001b[38;5;241m.\u001b[39moptimizations \u001b[38;5;241m=\u001b[39m [tf\u001b[38;5;241m.\u001b[39mlite\u001b[38;5;241m.\u001b[39mOptimize\u001b[38;5;241m.\u001b[39mDEFAULT]\n\u001b[0;32m      8\u001b[0m converter\u001b[38;5;241m.\u001b[39mrepresentative_dataset \u001b[38;5;241m=\u001b[39m representative_dataset_gen\n\u001b[1;32m----> 9\u001b[0m tflite_quant_model \u001b[38;5;241m=\u001b[39m \u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\lite\\python\\lite.py:929\u001b[0m, in \u001b[0;36m_export_metrics.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(convert_func)\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    928\u001b[0m   \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m--> 929\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_and_export_metrics(convert_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\lite\\python\\lite.py:908\u001b[0m, in \u001b[0;36mTFLiteConverterBase._convert_and_export_metrics\u001b[1;34m(self, convert_func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_conversion_params_metric()\n\u001b[0;32m    907\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mprocess_time()\n\u001b[1;32m--> 908\u001b[0m result \u001b[38;5;241m=\u001b[39m convert_func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    909\u001b[0m elapsed_time_ms \u001b[38;5;241m=\u001b[39m (time\u001b[38;5;241m.\u001b[39mprocess_time() \u001b[38;5;241m-\u001b[39m start_time) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m    910\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\lite\\python\\lite.py:1212\u001b[0m, in \u001b[0;36mTFLiteSavedModelConverterV2.convert\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1208\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_debug_info \u001b[38;5;241m=\u001b[39m _get_debug_info(\n\u001b[0;32m   1209\u001b[0m       _convert_debug_info_func(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trackable_obj\u001b[38;5;241m.\u001b[39mgraph_debug_info),\n\u001b[0;32m   1210\u001b[0m       graph_def)\n\u001b[1;32m-> 1212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_from_saved_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph_def\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\lite\\python\\lite.py:1096\u001b[0m, in \u001b[0;36mTFLiteConverterBaseV2._convert_from_saved_model\u001b[1;34m(self, graph_def)\u001b[0m\n\u001b[0;32m   1093\u001b[0m converter_kwargs\u001b[38;5;241m.\u001b[39mupdate(quant_mode\u001b[38;5;241m.\u001b[39mconverter_flags())\n\u001b[0;32m   1095\u001b[0m result \u001b[38;5;241m=\u001b[39m _convert_saved_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconverter_kwargs)\n\u001b[1;32m-> 1096\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimize_tflite_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_io\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperimental_new_quantizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\lite\\python\\convert_phase.py:215\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m    214\u001b[0m   report_error_message(\u001b[38;5;28mstr\u001b[39m(error))\n\u001b[1;32m--> 215\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m error \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\lite\\python\\convert_phase.py:205\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    204\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    206\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m ConverterError \u001b[38;5;28;01mas\u001b[39;00m converter_error:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m converter_error\u001b[38;5;241m.\u001b[39merrors:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\lite\\python\\lite.py:868\u001b[0m, in \u001b[0;36mTFLiteConverterBase._optimize_tflite_model\u001b[1;34m(self, model, quant_mode, quant_io)\u001b[0m\n\u001b[0;32m    866\u001b[0m   q_bias_type \u001b[38;5;241m=\u001b[39m quant_mode\u001b[38;5;241m.\u001b[39mbias_type()\n\u001b[0;32m    867\u001b[0m   q_allow_float \u001b[38;5;241m=\u001b[39m quant_mode\u001b[38;5;241m.\u001b[39mis_allow_float()\n\u001b[1;32m--> 868\u001b[0m   model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_quantize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_in_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_out_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_activations_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mq_bias_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_allow_float\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    871\u001b[0m m_in_type \u001b[38;5;241m=\u001b[39m in_type \u001b[38;5;28;01mif\u001b[39;00m in_type \u001b[38;5;28;01melse\u001b[39;00m _dtypes\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[0;32m    872\u001b[0m m_out_type \u001b[38;5;241m=\u001b[39m out_type \u001b[38;5;28;01mif\u001b[39;00m out_type \u001b[38;5;28;01melse\u001b[39;00m _dtypes\u001b[38;5;241m.\u001b[39mfloat32\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\lite\\python\\lite.py:612\u001b[0m, in \u001b[0;36mTFLiteConverterBase._quantize\u001b[1;34m(self, result, input_type, output_type, activations_type, bias_type, allow_float)\u001b[0m\n\u001b[0;32m    608\u001b[0m calibrate_quantize \u001b[38;5;241m=\u001b[39m _calibrator\u001b[38;5;241m.\u001b[39mCalibrator(result,\n\u001b[0;32m    609\u001b[0m                                             custom_op_registerers_by_name,\n\u001b[0;32m    610\u001b[0m                                             custom_op_registerers_by_func)\n\u001b[0;32m    611\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experimental_calibrate_only \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_new_quantizer:\n\u001b[1;32m--> 612\u001b[0m   calibrated \u001b[38;5;241m=\u001b[39m \u001b[43mcalibrate_quantize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalibrate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresentative_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_gen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experimental_calibrate_only:\n\u001b[0;32m    616\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m calibrated\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\lite\\python\\convert_phase.py:215\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m    214\u001b[0m   report_error_message(\u001b[38;5;28mstr\u001b[39m(error))\n\u001b[1;32m--> 215\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m error \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\lite\\python\\convert_phase.py:205\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    204\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    206\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m ConverterError \u001b[38;5;28;01mas\u001b[39;00m converter_error:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m converter_error\u001b[38;5;241m.\u001b[39merrors:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\lite\\python\\optimize\\calibrator.py:226\u001b[0m, in \u001b[0;36mCalibrator.calibrate\u001b[1;34m(self, dataset_gen)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;129m@convert_phase\u001b[39m(Component\u001b[38;5;241m.\u001b[39mOPTIMIZE_TFLITE_MODEL, SubComponent\u001b[38;5;241m.\u001b[39mCALIBRATE)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalibrate\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset_gen):\n\u001b[0;32m    218\u001b[0m   \u001b[38;5;124;03m\"\"\"Calibrates the model with specified generator.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \n\u001b[0;32m    220\u001b[0m \u001b[38;5;124;03m  Returns:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03m    dataset_gen: A generator that generates calibration samples.\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 226\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feed_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresize_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    227\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calibrator\u001b[38;5;241m.\u001b[39mCalibrate()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\lite\\python\\optimize\\calibrator.py:93\u001b[0m, in \u001b[0;36mCalibrator._feed_tensors\u001b[1;34m(self, dataset_gen, resize_input)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;03m\"\"\"Feed tensors to the calibrator.\"\"\"\u001b[39;00m\n\u001b[0;32m     91\u001b[0m initialized \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m dataset_gen():\n\u001b[0;32m     94\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sample, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sample[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36mrepresentative_dataset_gen\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrepresentative_dataset_gen\u001b[39m():\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mnum_calibration_steps\u001b[49m):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Get sample input data as a numpy array in a method of your choosing.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m        \u001b[38;5;28;01myield\u001b[39;00m [\u001b[38;5;28minput\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_calibration_steps' is not defined"
     ]
    }
   ],
   "source": [
    "def representative_dataset_gen():\n",
    "    for _ in range(num_calibration_steps):\n",
    "    # Get sample input data as a numpy array in a method of your choosing.\n",
    "       yield [input]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"./saved_model\")\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset_gen\n",
    "tflite_quant_model2 = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0064c4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cd7efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf0f6622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.772212790039615"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "319944/84816"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22922f95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fd2149",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdaca31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81049ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
